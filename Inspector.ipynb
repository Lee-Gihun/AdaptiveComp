{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from train_tools.utils import directory_setter\n",
    "import copy\n",
    "\n",
    "class InspectionHandler():\n",
    "    \"\"\"\n",
    "    Inspector for learned network. The forward function of model must return (outputs, mark).\n",
    "    outputs: (tensor) network output / mark: (tensor) marks consists of path indices.\n",
    "    \n",
    "    This module performs following inspections for different adaptive paths:\n",
    "    1) Computational Cost vs Accuracy Trade-off\n",
    "    2) Risk-Coverage Trade-off\n",
    "    3) Confidence(Softmax Response) & Entropy distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, Network, dataloaders, dataset_sizes, device='cuda:0', phase='test',\n",
    "                 num_path=1, path_cost=(1,), base_setting=True, use_small=False, save_path='./results/inspection/'):\n",
    "        \"\"\"\n",
    "        [args]     (int) num_path : the number of adaptive paths of inference \n",
    "                   (tuple) path_cost : relative cost of path flops w.r.t. total flops ex) (0.3, 0.7, 1.15)\n",
    "                                       default is None for 'no adaptive computation option'\n",
    "                   (bool) use_samll : True if using seperated small network as a single path \n",
    "        \"\"\"\n",
    "\n",
    "        self.Network = Network.to(device).eval()\n",
    "        self.dataloaders = dataloaders\n",
    "        self.dataset_sizes = dataset_sizes\n",
    "        self.device = device\n",
    "        self.phase = phase\n",
    "        self.num_path = num_path\n",
    "        self.path_cost = path_cost\n",
    "        self.use_small = 1 if use_small else 0\n",
    "        self.save_path = save_path\n",
    "        self.name = 'test' # default experiment name is 'test'\n",
    "        \n",
    "        if num_path != 1:\n",
    "            assert num_path == len(path_cost), 'number of paths should have corresponding cost!'\n",
    "        \n",
    "        # build base inspection results\n",
    "        self._result_dict_builder(phase=phase)\n",
    "        \n",
    "        if base_setting:\n",
    "            self._baseline_setter(phase=phase)\n",
    "            self._sr_dist_builder(phase=phase)\n",
    "            print('Baselines & SR distribution result has updated.\\n')\n",
    "        \n",
    "        \n",
    "    def inference(self, phase='test', exit_cond=None):\n",
    "        \"\"\"\n",
    "        Inference for given exit condition and dataset.\n",
    "        \n",
    "        [args]      (str) phase : use test or valid set 'valid' or 'test'\n",
    "                    (list or tuple) exit_cond : exiting threshold condition for the paths ex) (0.9, 0.97, 0.84)\n",
    "        \n",
    "        [returns]   (int) total_acc : total accuracy\n",
    "                    (list) path_acc : list of accuracy for paths\n",
    "                    (list) path_ratio : list of count ratio for paths\n",
    "                    (float) score : flops score for carried out inference\n",
    "        \"\"\"\n",
    "        self._condition_setter(exit_cond)\n",
    "        \n",
    "        path_correct, path_count = [0] * (self.num_path), [0] * (self.num_path)\n",
    "        size = self.dataset_sizes[phase]\n",
    "                 \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                # Inference from Network\n",
    "                outputs, mark = self._forward(inputs)\n",
    "                pred = self._prediction(outputs)\n",
    "                \n",
    "                path_mark = []\n",
    "                \n",
    "                if self.use_small:\n",
    "                    path_mark.append(mark == -1) # mark of smallnet is -1\n",
    "                    \n",
    "                for i in range(self.num_path-self.use_small):\n",
    "                    path_mark.append(mark == i)\n",
    "                \n",
    "                for i, marker in enumerate(path_mark):\n",
    "                    path_correct[i] += (pred[marker] == labels[marker]).sum().item()\n",
    "                    path_count[i] += marker.sum().item()\n",
    "                    \n",
    "                    \n",
    "        assert sum(path_count) == size, 'Total count must same with data count!'\n",
    "        \n",
    "        total_acc = round(sum(path_correct) / size, 4)\n",
    "        \n",
    "        path_acc, path_ratio = [], [] \n",
    "        \n",
    "        for i in range(self.num_path):\n",
    "            acc = -1 if path_count[i] == 0 else round((path_correct[i]/path_count[i]), 4) # assign -1 if no count\n",
    "            ratio = round((path_count[i]/size), 4)\n",
    "            path_acc.append(acc)\n",
    "            path_ratio.append(ratio)\n",
    "\n",
    "        flops_score = self._flops_checker(path_ratio)\n",
    "        \n",
    "        return total_acc, path_acc, path_ratio, flops_score\n",
    "\n",
    "    \n",
    "    def grid_inspector(self, start_cond, grid=0.01):\n",
    "        \"\"\"\n",
    "        Inspection for grid condition.\n",
    "        \n",
    "        [args]      (list or tuple) start_cond: start cond of condition ragne for paths ex) [0.5, 0.55, 0.7]\n",
    "        \"\"\" \n",
    "        assert len(start_cond) == self.num_path-1, 'condition should 1 less than num_path!'\n",
    "        \n",
    "        condition_range = []\n",
    "        condition_elem_num = []\n",
    "        \n",
    "        for start in start_cond:\n",
    "            # build condition range for paths\n",
    "            cond_size = int((1.0-start)//grid)\n",
    "            cond_list = [start + x*grid for x in range(cond_size)]\n",
    "\n",
    "            # confirm final threshold as 1.0\n",
    "            if cond_list[-1] != 1:\n",
    "                cond_list.append(1.0)\n",
    "            \n",
    "            condition_range.append(cond_list)\n",
    "            condition_elem_num.append(len(cond_list))\n",
    "        \n",
    "        # build grid condition set\n",
    "        condition_set = self._grid_cond_builder(condition_set=[], condition_range=condition_range)\n",
    "        \n",
    "        print('Starting Grid Inspection...\\n')\n",
    "\n",
    "        for i, exit_cond in enumerate(condition_set):\n",
    "            total_acc, path_acc, path_ratio, flops_score = self.inference(phase=self.phase, exit_cond=exit_cond)\n",
    "            self._result_dict_updater(exit_cond, total_acc, path_acc, path_ratio, flops_score)\n",
    "            \n",
    "            if (i%100 == 0) and (i > 0):\n",
    "                print('Grid Inspection (%d/%d)'%(i, len(condition_set)))\n",
    "    \n",
    "    \n",
    "    def sr_dist_inspector(self, path_idx, phase='test'):\n",
    "        \"\"\"\n",
    "        Inspects logit distribution and top-10 entropy distribution for given path\n",
    "        \n",
    "        [args]      (int) path_idx : 0 <= path index. < num_paths \n",
    "                    (str) phase : use test or valid set 'valid' or 'test'\n",
    "                    \n",
    "        [returns]   (tuple) (max_sr_co, max_sr_inco) : lists of maximum softmax response for correct/incorrect samples\n",
    "                    (tuple) (entropy_co, entropy_inco) : lists of entropy of top-10 softmax response for correct/incorrect samples\n",
    "        \"\"\"\n",
    "        # Set full path condition\n",
    "        condition = self._fullcond_setter(path_idx)\n",
    "        self._condition_setter(condition)\n",
    "        \n",
    "        max_sr_co, max_sr_inco = [], []\n",
    "        entropy_co, entropy_inco = [], []\n",
    "        \n",
    "        size = self.dataset_sizes[phase]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # inference & prediction\n",
    "                outputs, _ = self._forward(inputs)\n",
    "                pred = self._prediction(outputs)\n",
    "                \n",
    "                # SR(softmax response)\n",
    "                soft_out = F.softmax(outputs, dim=1)\n",
    "                \n",
    "                # maximum softmax response\n",
    "                max_sr, _ = soft_out.max(dim=1) # maximum SR\n",
    "                \n",
    "                # top-10 entropy of softmax resnponse\n",
    "                top10_out, _ = torch.topk(soft_out, 10) # top-10 SR\n",
    "                entropy = (top10_out * -top10_out.log()).sum(dim=1) # top-10 SR entropy\n",
    "\n",
    "                # Get correct/incorrect tensor\n",
    "                co_tensor = pred == labels\n",
    "                inco_tensor = pred != labels\n",
    "                \n",
    "                # get values for correct/incorrect samples\n",
    "                co_sr, inco_sr = max_sr[co_tensor].tolist(), max_sr[inco_tensor].tolist()\n",
    "                co_entropy, inco_entropy = entropy[co_tensor].tolist(), entropy[inco_tensor].tolist()\n",
    "\n",
    "                # update max_sr list\n",
    "                max_sr_co += co_sr\n",
    "                max_sr_inco += inco_sr\n",
    "                \n",
    "                # update entropy list\n",
    "                entropy_co += co_entropy\n",
    "                entropy_inco += inco_entropy\n",
    "\n",
    "        return (max_sr_co, max_sr_inco), (entropy_co, entropy_inco)\n",
    "    \n",
    "    \n",
    "    def _sr_dist_builder(self, phase='test'):\n",
    "        \"\"\"\n",
    "        Updates sr_dist for each paths\n",
    "        \n",
    "        [args]      (int) path_idx : 0 <= path index. < num_paths \n",
    "                    (str) phase : use test or valid set 'valid' or 'test'\n",
    "        \"\"\"\n",
    "        for path_idx in range(self.num_path):\n",
    "            (max_sr_co, max_sr_inco), (entropy_co, entropy_inco) = self.sr_dist_inspector(path_idx, phase)\n",
    "            self.result_dict['max_sr_dist_'+str(path_idx)][0].append(max_sr_co)\n",
    "            self.result_dict['max_sr_dist_'+str(path_idx)][1].append(max_sr_inco)\n",
    "            self.result_dict['entropy_dist_'+str(path_idx)][0].append(entropy_co)\n",
    "            self.result_dict['entropy_dist_'+str(path_idx)][1].append(entropy_inco)\n",
    "            \n",
    "            \n",
    "    def _forward(self, x):\n",
    "        \"\"\"\n",
    "        Inference for a single batch. \n",
    "        \n",
    "        [returns]   (Tensor) outputs : inference result tensor\n",
    "                    (Tensor) mark : mark tensor for paths. None if no adaptive path exits.\n",
    "        \"\"\"\n",
    "        if self.path_cost is not None:\n",
    "            outputs, mark = self.Network(x)\n",
    "        else:\n",
    "            outputs, mark = self.Network(x), None\n",
    "        \n",
    "        return outputs, mark\n",
    "             \n",
    "    \n",
    "    def _prediction(self, outputs):\n",
    "        \"\"\"\n",
    "        Prediction for a single batch inference result.\n",
    "        \n",
    "        [returns]   (Tensor) max_logits : maximum softmax output value tensor\n",
    "                    (Tensor) pred : prediction result tensor\n",
    "        \"\"\"\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        return pred\n",
    "\n",
    "    \n",
    "    def _flops_checker(self, path_ratio):\n",
    "        \"\"\"\n",
    "        Calculates total flops. Regards main network cost itself as 1.0\n",
    "        \n",
    "        [args]      (list) path_ratio: path ratio for paths\n",
    "        \n",
    "        [returns]   (float) score : flops score for given ratio list\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        for i, cost in enumerate(self.path_cost):\n",
    "            score += (cost * path_ratio[i])\n",
    "\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def _condition_setter(self, exit_cond):\n",
    "        \"\"\"\n",
    "        Sets exiting threshold for each paths in Network.\n",
    "        \n",
    "        [args]      (list or tuple) cond: exiting threshold condition for the paths ex) (0.9, 0.97, 0.84)\n",
    "        \"\"\"\n",
    "        assert (self.num_path-1) == len(exit_cond), 'the number of exit_cond should 1 less than the number of paths!'\n",
    "        self.Network.condition_updater(exit_cond)\n",
    "\n",
    "\n",
    "    \n",
    "    def _fullcond_setter(self, path_idx):\n",
    "        \"\"\"\n",
    "        Returns full exiting condition for given path.\n",
    "        \n",
    "        [args]      (int) path_idx: path index (0 <= path_idx < num_paths)\n",
    "        \n",
    "        [returns]   (list) condition : condition list for corresponding path\n",
    "        \"\"\"\n",
    "        condition = [1 * (self.num_path-1)] # coindition length should 1 less than path length\n",
    "        if path_idx < self.num_path-1:\n",
    "            condition[path_idx] = 0 # sets threshold as 0 for full exiting ex) [1,1,1,0,1]\n",
    "        \n",
    "        return condition\n",
    "    \n",
    "\n",
    "    def _grid_cond_builder(self, condition_set, condition_range):\n",
    "        \"\"\"\n",
    "        Recursively builds list of exit_cond for given condition ranges.\n",
    "        \n",
    "        [args]      (list) condition_set : list of exit_cond. empty list at first\n",
    "                    (list) condition_range : list of condition range tuples\n",
    "                    \n",
    "        [returns]   (list) condition_set : list of exit_cond (recursive result)\n",
    "        \"\"\"\n",
    "        if len(condition_range) == 0: \n",
    "            # return if no more condition range exists.\n",
    "            return condition_set\n",
    "\n",
    "        else:\n",
    "            if not condition_set: # if condition_set is empty (initial state)\n",
    "                # append conditions for empty list\n",
    "                for cond in condition_range[-1]:\n",
    "                    condition_set.append([cond])\n",
    "                condition_set = self._grid_cond_builder(condition_set, condition_range[:-1])\n",
    "            \n",
    "            else:\n",
    "                new_condition_set = []\n",
    "                \n",
    "                # for condition range of a specific path\n",
    "                for i in range(len(condition_range[-1])):\n",
    "                    condition_subset = copy.deepcopy(condition_set) # make sure 1 to 1 mapping\n",
    "                    # for conditions which already built\n",
    "                    for j in range(len(condition_subset)):\n",
    "                        condition_subset[j].insert(0, condition_range[-1][i]) # insert path condition\n",
    "                    new_condition_set += condition_subset\n",
    "\n",
    "                condition_set = new_condition_set\n",
    "                condition_set = self._grid_cond_builder(condition_set, condition_range[:-1])\n",
    "\n",
    "        return condition_set\n",
    "\n",
    "    \n",
    "    def _baseline_setter(self, phase='test'):\n",
    "        \"\"\"\n",
    "        Updates baseline accuracy for each path.\n",
    "        \n",
    "        [args]      (str) phase : use test or valid set 'valid' or 'test '\n",
    "        \"\"\"\n",
    "        baseline = []\n",
    "        \n",
    "        for i in range(self.num_path): \n",
    "            condition = self._fullcond_setter(i)\n",
    "            accuracy, _, _, _ = self.inference(phase=phase, exit_cond=condition)\n",
    "            baseline.append(accuracy)\n",
    "        \n",
    "        self.result_dict['baseline'] = baseline    \n",
    "    \n",
    "    \n",
    "    def _result_dict_builder(self, phase):\n",
    "        \"\"\"\n",
    "        Builds a dictionary to save results\n",
    "        \"\"\"\n",
    "        result_dict = {\n",
    "            'phase' : phase,\n",
    "            'total_acc': [],\n",
    "            'flops_score' : []\n",
    "        }\n",
    "        \n",
    "        result_dict['path_cost'] = self.path_cost\n",
    "        \n",
    "        for i in range(self.num_path):\n",
    "            result_dict['path_acc_' + str(i)] = []\n",
    "            result_dict['path_ratio_' + str(i)] = []\n",
    "            result_dict['path_cond_' + str(i)] = []\n",
    "            result_dict['max_sr_dist_'+ str(i)] = [[], []]  # [co_dist, inco_dist]\n",
    "            result_dict['entropy_dist_'+ str(i)] = [[], []] # [co_dist, inco_dist]\n",
    "\n",
    "        self.result_dict = result_dict\n",
    "            \n",
    "    \n",
    "    def _result_dict_updater(self, exit_cond, total_acc, path_acc, path_ratio, flops_score):\n",
    "        \"\"\"\n",
    "        Updates inference result to result_dict\n",
    "        \"\"\"        \n",
    "        assert len(path_acc) == len(path_ratio), 'path number should same for accuracy and ratio!'\n",
    "\n",
    "        self.result_dict['total_acc'].append(total_acc)\n",
    "        self.result_dict['flops_score'].append(flops_score)\n",
    "        \n",
    "        for i in range(self.num_path):\n",
    "            self.result_dict['path_acc_'+str(i)].append(path_acc[i])\n",
    "            self.result_dict['path_ratio_'+str(i)].append(path_ratio[i])\n",
    "            \n",
    "            if i < self.num_path-1:\n",
    "                path_cond = exit_cond[i]\n",
    "            else:\n",
    "                path_cond = -1\n",
    "            \n",
    "            self.result_dict['path_cond_'+str(i)].append(path_cond)\n",
    "    \n",
    "    \n",
    "    def plotter(self, make_dir=True):\n",
    "        \"\"\"\n",
    "        explanation\n",
    "        \n",
    "        [args]      (type) name: \n",
    "\n",
    "        [returns]   (type) name : \n",
    "        \"\"\"\n",
    "        save_path = self.save_path + self.name\n",
    "        directory_setter(save_path, make_dir)\n",
    "        fig = None\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.adaptivecomp.utils import LogitCond, SL_Pair\n",
    "from data_utils import cifar_100_setter\n",
    "from models.ResNet import resnet101, resnet18\n",
    "\n",
    "dataloaders, dataset_sizes = cifar_100_setter(root='./data/cifar100')\n",
    "\n",
    "smallnet = resnet18(num_classes=100)\n",
    "smallnet.load_state_dict(torch.load('./results/trained_models/ResNet18_ce_acc/trained_model.pth'))\n",
    "largenet = resnet101(num_classes=100)\n",
    "largenet.load_state_dict(torch.load('./results/trained_models/ResNet101_ce/trained_model.pth'))\n",
    "\n",
    "model = SL_Pair(smallnet, largenet, exit_cond=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baselines & SR distribution result has updated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspector = InspectionHandler(model, dataloaders, dataset_sizes, num_path=2, path_cost=(0.5, 1.0), use_small=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Inspection...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspector.grid_inspector([0.7], grid=0.025)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
